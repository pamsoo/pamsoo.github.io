---
title: Manila × CephFS × NFS-Ganesha 구성
tags: manila cephfs nfs-ganesha
---

이 포스트는 OpenStack Queens에서 파일 스토리지 서비스 제공을 담당하는 Manila를 구성하는 내용을 담고 있습니다. 실제 테스트 환경에서는 OpenStack Ansible을 사용하여 OpenStack Queens를 배포하였습니다. 다만 OpenStack Ansible (stable/queens)는 Manila 설치를 지원하지 않아서 수작업으로 설치합니다. Manila 설치와 관련된 파이썬 가상 환경 설정, 서비스 설정 파일 작성, Systemd Unit 파일 작성 등은 OpenStack Ansible을 참고하였습니다.

Manila Share의 백엔드로는 CephFS를 사용하고, 파일 스토리지 서비스는 NFS로 제공합니다. CephFS를 NFS로 서비스하기 위해 NFS-Ganesha를 사용하였으며, NFS-Ganesha의 고가용성을 확보하기 위하여 작업한 내용을 포함하고 있습니다.

# Manila 설치

## 사전 작업

1. Database 생성

    - DBMS 접속

        ```
        mysql -uroot -p
        ```

    - manila DB 생성

        ```
        CREATE DATABASE manila;
        ```

    - DB 권한 설정

        아래 코드 중 ___`MANILA_DBPASS`___ 는 배포하는 환경에 맞게 변경해야 합니다.

        ```
        GRANT ALL PRIVILEGES ON manila.* TO 'manila'@'localhost' \
          IDENTIFIED BY 'MANILA_DBPASS';
        GRANT ALL PRIVILEGES ON manila.* TO 'manila'@'%' \
          IDENTIFIED BY 'MANILA_DBPASS';
        ```

2. OpenStack 서비스 설정

    - CLI 명령을 위해 admin 환경 파일 적용

        ```
        source openrc
        ```

    - 사용자 및 서비스 생성

        ```
        openstack user create --domain default --password-prompt manila
        openstack role add --project service --user manila admin
        ```
        ```
        openstack service create --name manila \
          --description "OpenStack Shared File Systems" share
        ```
        ```
        openstack service create --name manilav2 \
          --description "OpenStack Shared File Systems V2" sharev2
        ```

    - 서비스 Endpoint 생성

        아래 코드 중 ___`CONTROLLER`___ 는 배포하는 환경에 맞게 변경해야 합니다.

        ```
        openstack endpoint create --region RegionOne \
          share public http://CONTROLLER:8786/v1/%\(tenant_id\)s
        openstack endpoint create --region RegionOne \
          share internal http://CONTROLLER:8786/v1/%\(tenant_id\)s
        openstack endpoint create --region RegionOne \
          share admin http://CONTROLLER:8786/v1/%\(tenant_id\)s
        ```
        ```
        openstack endpoint create --region RegionOne \
          sharev2 public http://CONTROLLER:8786/v2/%\(tenant_id\)s
        openstack endpoint create --region RegionOne \
          sharev2 internal http://CONTROLLER:8786/v2/%\(tenant_id\)s
        openstack endpoint create --region RegionOne \
          sharev2 admin http://CONTROLLER:8786/v2/%\(tenant_id\)s
        ```

3. 메시지큐 설정

    Manila 컴포넌트는 OpenStack의 다른 컴포넌트와 마찬가지로 메시지큐를 사용합니다. RabbitMQ를 사용하는 경우 아래와 같은 단계로 사용자와 가상호스트를 생성하고, 권한을 부여합니다.
    아래 코드 중 ___`MANILA_MQPASS`___ 는 배포하는 환경에 맞게 변경해야 합니다.

    ```
    rabbitmqctl add_user manila MANILA_MQPASS
    ```
    ```
    rabbitmqctl add_vhost /manila
    ```
    ```
    rabbitmqctl set_permissions -p /manila manila ".*" ".*" ".*"
    ```

## 본 작업

Manila 소스 코드를 사용하여 파이썬 가상 환경(Virtual Environment)에 설치합니다. API를 제공하는 manila-api 서비스는 uWSGI를 사용합니다.

1. Manila 설치 준비

    - 파이썬 가상 환경 생성

        여기서 사용하는 가상 환경 경로는 `/openstack/venvs/manila`입니다. 이후의 각종 설정 파일에서도 이 경로를 사용합니다. 배포하는 환경에 맞게 변경할 수 있습니다.

        ```
        pip install virtualenv
        ```
        ```
        virtualenv /openstack/venvs/manila
        source /openstack/venvs/manila/bin/activate
        ```

    - 필수 패키지 설치

        ```
        pip install uwsgi PyMySQL
        ```

2. Manila 설치

    - Manila 소스 코드 다운로드 및 설치

        ```
        cd /opt
        git clone -b stable/queens https://opendev.org/openstack/manila
        pip install -r manila/requirements.txt manila
        ```

    - 서비스 전용 사용자 생성

        ```
        useradd -r -s /bin/false -d /noexistent -c "manila system user" manila
        ```

    - 사용자 sudoer 권한 설정

        ```
        vi /etc/sudoers.d/manila_sudoers
        ```

        > ```
        > Defaults:manila !requiretty
        > Defaults:manila secure_path="/openstack/venvs/manila/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
        > 
        > manila ALL = (root) NOPASSWD: /openstack/venvs/manila/bin/manila-rootwrap
        > ```

3. CephFS 드라이버 수정

    manila-share가 사용하는 CephFS 드라이버는 NFS를 사용하는 경우 Export 설정을 위해서 NFS-Ganesha 호스트에 SSH로 접속합니다. 그러나 세션 타임아웃 기능이 빠져있어 NFS-Ganesha 호스트가 Failover된 경우에도 SSH 접속을 새로 하지 않고 세션을 계속 유지하고 있습니다. NFS-Ganesha 호스트가 Failover되는 경우에 기존 SSH 세션이 자동으로 끊어지도록 CephFS 드라이버를 아래와 같이 수정합니다.

    ```
    vi /openstack/venvs/manila/lib/python2.7/site-packages/manila/share/drivers/cephfs/driver.py
    ```
    > ```
    > ...
    > class NFSProtocolHelper(ganesha.GaneshaNASHelper2):
    > 
    >     shared_data = {}
    >     supported_protocols = ('NFS',)
    > 
    >     def __init__(self, execute, config_object, **kwargs):
    >         if config_object.cephfs_ganesha_server_is_remote:
    >             execute = ganesha_utils.SSHExecutor(
    >                 config_object.cephfs_ganesha_server_ip, 22, # None,
    >                 config_object.ssh_conn_timeout,
    >                 config_object.cephfs_ganesha_server_username,
    >                 password=config_object.cephfs_ganesha_server_password,
    >                 privatekey=config_object.cephfs_ganesha_path_to_private_key)
    >         else:
    >             execute = ganesha_utils.RootExecutor(execute)
    > ...
    > ```


3. Manila 설정

    - 동작 디렉토리 생성

        `mkdir` 후 `chown`으로 같은 결과를 얻을 수 있지만, `install` 명령을 사용하여 한번에 작업할 수 있습니다.

        ```
        install -d -m 0755 -o manila -g manila /var/lib/manila \
                                               /var/lock/manila \
                                               /var/log/manila \
                                               /var/run/manila-api
        ```

        `/var/run`, `/var/lock` 디렉토리는 각각 `../run`, `../run/lock`으로 링크되어 있으며, `/run`은 `df` 명령으로 확인해보면 임시 파일시스템(tmpfs)에 마운트되어 있습니다. 따라서 시스템을 재부팅하면 특별히 조치하지 않는 이상 `/var/run`과 `/var/lock` 디렉토리 안의 모든 파일이 삭제됩니다. 시스템이 재부팅되는 경우 수행해야 할 디렉토리 생성 과정을 자동화하기 위하여 아래와 같이 설정합니다.

        ```
        vi /etc/tmpfiles.d/openstack-manila-api.conf
        ```
        > ```
        > D /var/run/manila-api 2755 manila manila
        > ```

        ```
        vi /etc/tmpfiles.d/openstack-manila-share.conf
        ```
        > ```
        > D /var/lock/manila 2755 manila manila
        > ```
    
    - uWSGI 설정

        ```
        mkdir -p /etc/uwsgi
        vi /etc/uwsgi/manila-api.ini
        ```
        > ```
        > [uwsgi]
        > uid = manila
        > gid = manila
        > 
        > virtualenv = /openstack/venvs/manila
        > wsgi-file = /openstack/venvs/manila/bin/manila-wsgi
        > http = 0.0.0.0:8786
        > 
        > master = true
        > enable-threads = true
        > processes = 16
        > threads = 1
        > exit-on-reload = false
        > die-on-term = true
        > lazy-apps = true
        > add-header = Connection: close
        > buffer-size = 65535
        > thunder-lock = true
        > logfile-chmod = 644
        > pidfile = /var/run/manila-api/manila-api.pid
        > 
        > # Avoid filling up the logs with health check requests from haproxy.
        > route-user-agent = ^osa-haproxy-healthcheck$ donotlog:
        > ```

    - 설정 파일 디렉토리 생성

        ```
        cp -r /opt/manila/etc/manila /etc/
        chown manila:manila /etc/manila
        chown root:manila /etc/manila/*
        chown -R root:root /etc/manila/rootwrap.d
        ```
    
    - Rootwrap 설정

        `exec_dirs` 항목에 파이썬 가상 환경 경로로 만든 `/openstack/venvs/manila/bin`을 추가합니다. 그 밖의 내용은 원본 파일과 같습니다.

        ```
        vi /etc/manila/rootwrap.conf
        ```
        > ```
        > # Configuration for manila-rootwrap
        > # This file should be owned by (and only-writeable by) the root user
        > 
        > [DEFAULT]
        > # List of directories to load filter definitions from (separated by ',').
        > # These directories MUST all be only writeable by root !
        > filters_path=/etc/manila/rootwrap.d,/usr/share/manila/rootwrap
        > 
        > # List of directories to search executables in, in case filters do not
        > # explicitly specify a full path (separated by ',')
        > # If not specified, defaults to system PATH environment variable.
        > # These directories MUST all be only writeable by root !
        > exec_dirs=/openstack/venvs/manila/bin,/sbin,/usr/sbin,/bin,/usr/bin,/usr/local/sbin,/usr/local/bin
        > 
        > # Enable logging to syslog
        > # Default value is False
        > use_syslog=False
        > 
        > # Which syslog facility to use.
        > # Valid values include auth, authpriv, syslog, user0, user1...
        > # Default value is 'syslog'
        > syslog_log_facility=syslog
        > 
        > # Which messages to log.
        > # INFO means log all usage
        > # ERROR means only log unsuccessful attempts
        > syslog_log_level=ERROR
        > ```

    - manila.conf 설정

        ```
        vi /etc/manila/manila.conf
        ```
        > ```
        > [DEFAULT]
        > use_journal = False
        > # Disable stderr logging
        > use_stderr = False
        > debug = False
        > fatal_deprecations = False
        > my_ip = MANILA_NODE_IP  # Internal
        > 
        > default_share_type = nfs
        > share_name_template = share-%s
        > 
        > osapi_share_workers = 16
        > 
        > rootwrap_config = /etc/manila/rootwrap.conf
        > api_paste_config = /etc/manila/api-paste.ini
        > auth_strategy = keystone
        > 
        > ## RabbitMQ RPC
        > executor_thread_pool_size = 64
        > rpc_response_timeout = 60
        > 
        > transport_url = rabbit://manila:MANILA_MQPASS@RABBITMQ_IP1:5672,manila:MANILA_MQPASS@RABBITMQ_IP2:5672,manila:MANILA_MQPASS@RABBITMQ_IP3:5672//manila
        > 
        > ## Quota
        > quota_shares = -1
        > quota_snapshots = -1
        > quota_gigabytes = -1
        > quota_snapshot_gigabytes = -1
        > quota_share_networks = -1
        > 
        > os_region_name = RegionOne
        > 
        > storage_availability_zone = nova
        > 
        > client_socket_timeout = 900
        > 
        > enabled_share_protocols = NFS
        > 
        > ssh_conn_timeout = 30
        > ssh_min_pool_conn = 1
        > ssh_max_pool_conn = 1
        > 
        > enabled_share_backends = cephfsnfs
        > 
        > # All given backend(s)
        > [cephfsnfs]
        > driver_handles_share_servers = False
        > share_backend_name = CEPHFSNFS
        > share_driver = manila.share.drivers.cephfs.driver.CephFSDriver
        > cephfs_protocol_helper_type = NFS
        > cephfs_conf_path = /etc/ceph/ceph.conf
        > cephfs_auth_id = manila
        > cephfs_cluster_name = ceph
        > cephfs_enable_snapshots = False
        > ganesha_rados_store_enable = True
        > ganesha_rados_store_pool_name = cephfs_data
        > cephfs_ganesha_server_is_remote = True
        > cephfs_ganesha_server_ip = GANESHA_IP  # will be exported
        > cephfs_ganesha_server_username = root
        > cephfs_ganesha_path_to_private_key = /etc/manila/.ssh/manila_rsa
        > 
        > [database]
        > connection = mysql+pymysql://manila:MANILA_DBPASS@DB_IP/manila?charset=utf8
        > 
        > [oslo_messaging_rabbit]
        > ssl = False
        > 
        > [oslo_messaging_notifications]
        > driver = messagingv2
        > transport_url = rabbit://manila:MANILA_MQPASS@RABBITMQ_IP1:5672,manila:MANILA_MQPASS@RABBITMQ_IP2:5672,manila:MANILA_MQPASS@RABBITMQ_IP3:5672//manila
        > 
        > [oslo_concurrency]
        > lock_path = /var/lock/manila
        > 
        > [profiler]
        > enabled = False
        > trace_sqlalchemy = False
        > hmac_keys = MANIAL_HMAC_KEY
        > 
        > [keystone_authtoken]
        > insecure = False
        > auth_type = password
        > auth_url = http://CONTROLLER:35357
        > auth_uri = http://CONTROLLER:5000
        > project_domain_id = default
        > user_domain_id = default
        > project_name = service
        > username = manila
        > password = MANILA_SVCPASS
        > region_name = RegionOne
        > 
        > memcached_servers = MEMCACHED_IP1:11211,MEMCACHED_IP2:11211,MEMCACHED_IP3:11211
        > 
        > token_cache_time = 300
        > 
        > # if your memcached server is shared, use these settings to avoid cache poisoning
        > memcache_security_strategy = ENCRYPT
        > memcache_secret_key = MEMCACHED_SECRET_KEY
        > 
        > [neutron]
        > auth_type = password
        > auth_url = http://CONTROLLER:35357/v3
        > region_name = RegionOne
        > project_domain_id = default
        > user_domain_id = default
        > project_name = service
        > username = neutron
        > password = NEUTRON_SVCPASS
        > endpoint_type = internal
        > 
        > [nova]
        > auth_type = password
        > auth_url = http://CONTROLLER:35357/v3
        > region_name = RegionOne
        > project_domain_id = default
        > user_domain_id = default
        > project_name = service
        > username = nova
        > password = NOVA_SVCPASS
        > endpoint_type = internal
        > 
        > [cinder]
        > auth_type = password
        > auth_url = http://CONTROLLER:35357/v3
        > region_name = RegionOne
        > project_domain_id = default
        > user_domain_id = default
        > project_name = service
        > username = cinder
        > password = CINDER_SVCPASS
        > endpoint_type = internal
        > ```

    - Ganesha 노드 SSH 통신을 위한 Key 생성 및 설치
    
        manila-share는 NFS-Ganesha가 원격 호스트에 구성되어 있는 경우에 SSH로 접속하여 NFS Export 정보를 설정합니다. 패스워드를 사용할 수도 있지만 여기서는 키 파일을 사용합니다.

        ```
        install -d -m 0700 -o manila -g manila /etc/manila/.ssh
        sudo -u manila bash -c "ssh-keygen -f /etc/manila/.ssh/manila_rsa  -N ''"
        ```
        ```
        ssh-copy-id -i /etc/manila/.ssh/manila_rsa.pub GANESHA_IP
        ```
    
    - Systemd Unit 파일 생성
    
        ```
        vi /etc/systemd/system/manila-api.service
        ```
        > ```
        > [Unit]
        > Description=manila openstack service
        > After=syslog.target
        > After=network.target
        > 
        > [Service]
        > Type=simple
        > User=manila
        > Group=manila
        > 
        > ExecStart=/openstack/venvs/manila/bin/uwsgi --ini /etc/uwsgi/manila-api.ini --logto /var/log/manila/manila-api.log
        > ExecReload=/openstack/venvs/manila/bin/uwsgi --reload /var/run/manila-api/manila-api.pid
        > 
        > # Give a reasonable amount of time for the server to start up/shut down
        > TimeoutSec=120
        > Restart=on-failure
        > RestartSec=2
        > 
        > # This creates a specific slice which all manila services will operate from
        > #  The accounting options give us the ability to see resource usage through
        > #  the `systemd-cgtop` command.
        > Slice=manila.slice
        > CPUAccounting=true
        > BlockIOAccounting=true
        > MemoryAccounting=false
        > TasksAccounting=true
        > 
        > [Install]
        > WantedBy=multi-user.target
        > ```

        ```
        vi /etc/systemd/system/manila-scheduler.service
        ```
        > ```
        > [Unit]
        > Description=manila openstack service
        > After=syslog.target
        > After=network.target
        > 
        > [Service]
        > Type=simple
        > User=manila
        > Group=manila
        > 
        > ExecStart=/openstack/venvs/manila/bin/manila-scheduler --log-file=/var/log/manila/manila-scheduler.log
        > ExecReload=/bin/kill -HUP $MAINPID
        > 
        > # Give a reasonable amount of time for the server to start up/shut down
        > TimeoutSec=120
        > Restart=on-failure
        > RestartSec=2
        > 
        > # This creates a specific slice which all manila services will operate from
        > #  The accounting options give us the ability to see resource usage through
        > #  the `systemd-cgtop` command.
        > Slice=manila.slice
        > CPUAccounting=true
        > BlockIOAccounting=true
        > MemoryAccounting=false
        > TasksAccounting=true
        > 
        > [Install]
        > WantedBy=multi-user.target
        > ```

        ```
        vi /etc/systemd/system/manila-share.service
        ```
        > ```
        > [Unit]
        > Description=manila openstack service
        > After=syslog.target
        > After=network.target
        > 
        > [Service]
        > Type=simple
        > User=manila
        > Group=manila
        > 
        > ExecStart=/openstack/venvs/manila/bin/manila-share --log-file=/var/log/manila/manila-share.log
        > ExecReload=/bin/kill -HUP $MAINPID
        > 
        > # Give a reasonable amount of time for the server to start up/shut down
        > TimeoutSec=120
        > Restart=on-failure
        > RestartSec=2
        > 
        > # This creates a specific slice which all manila services will operate from
        > #  The accounting options give us the ability to see resource usage through
        > #  the `systemd-cgtop` command.
        > Slice=manila.slice
        > CPUAccounting=true
        > BlockIOAccounting=true
        > MemoryAccounting=false
        > TasksAccounting=true
        > 
        > [Install]
        > WantedBy=multi-user.target
        > ```
        
        ```
        systemctl daemon-reload
        ```

# Ceph Client 설치

Ceph 설치는 이 포스트에서 다루고자 하는 범위를 벗어나는 내용이므로, Ceph Cluster 구성과 CephFS를 사용하기 위한 Pool 생성은 이미 하였다고 가정합니다.

1. YUM Repository 설정

    ```
    vi /etc/yum.repos.d/ceph.repo
    ```
    > ```
    > [ceph]
    > baseurl = https://download.ceph.com/rpm-luminous/el7/$basearch
    > enabled = 1
    > gpgcheck = 1
    > gpgkey = https://download.ceph.com/keys/release.asc
    > name = Ceph packages for $basearch
    > priority = 50
    > 
    > [ceph-noarch]
    > baseurl = https://download.ceph.com/rpm-luminous/el7/noarch
    > enabled = 1
    > gpgcheck = 1
    > gpgkey = https://download.ceph.com/keys/release.asc
    > name = Ceph noarch packages
    > priority = 50
    > ```

2. 패키지 설치

    ```
    yum install --nogpgcheck ceph-common-12.2.12
    ```

    Manila는 파이썬 가상 환경(Virtual Environment)에 설치한 반면 Ceph 패키지는 실제 환경(?)에 설치되기 때문에 특별히 조치하지 않으면 Manila는 Ceph 라이브러리를 사용하지 못합니다. Manila가 설치된 가상 환경에서 Ceph 라이브러리를 사용할 수 있도록 아래와 같이 링크를 걸어 줍니다.

    ```
    cd /openstack/venvs/manila/lib/python2.7/site-packages
    ```
    ```
    ln -s /usr/lib64/python2.7/site-packages/cephfs.so
    ln -s /usr/lib64/python2.7/site-packages/rados.so
    ln -s /usr/lib64/python2.7/site-packages/rbd.so
    ln -s /lib/python2.7/site-packages/ceph_argparse.py
    ln -s /lib/python2.7/site-packages/ceph_daemon.py
    ln -s /lib/python2.7/site-packages/ceph_volume_client.py
    ```

3. Ceph 설정

    - Ceph 접근 권한 설정
      
        Manila가 Ceph 클러스터에 접근할 수 있도록 권한을 설정합니다. 이 단계는 관리자 권한으로 `ceph` 명령을 실행할 수 있는 곳에서 진행합니다.

        ```
        read -d '' MON_CAPS << EOF
        allow r,
        allow command "auth del",
        allow command "auth caps",
        allow command "auth get",
        allow command "auth get-or-create"
        EOF

        ceph auth get-or-create client.manila -o /etc/ceph/ceph.client.manila.keyring \
        mds 'allow *' \
        osd 'allow rw' \
        mon "$MON_CAPS"
        ```

        Manila 서비스용 Keyring 파일과 Admin Keyring 파일을 Manila가 설치된 호스트의 `/etc/ceph` 디렉토리로 복사합니다. 여기서는 `scp`를 사용하여 복사합니다.

        ```
        scp /etc/ceph/ceph.client.manila.keyring root@MANILA_NODE:/etc/ceph
        scp /etc/ceph/ceph.client.admin.keyring root@MANILA_NODE:/etc/ceph
        ``` 

    - Ceph 설정 파일 생성

        ```
        vi /etc/ceph/ceph.conf
        ```
        > ```
        > [global]
        > public network = CEPH_PUBLIC_NET
        > cluster network = CEPH_CLUSTER_NET
        > fsid = CEPH_FSID
        > 
        > mon host = CEPH_MON_IP1,CEPH_MON_IP2,CEPH_MON_IP3
        > mon initial members = CEPH_MON_HOSTNAME1,CEPH_MON_HOSTNAME2,CEPH_MON_HOSTNAME3
        > 
        > [client.manila]
        > client mount uid = 0
        > client mount gid = 0
        > log file = /var/log/ceph/client.manila.log
        > #admin socket = /var/run/manila/ceph-$name.$pid.asok
        > keyring = /etc/ceph/ceph.client.manila.keyring
        > 
        > [client.admin]
        > keyring = /etc/ceph/ceph.client.admin.keyring
        > ```

# NFS-Ganesha 구성

NFS-Ganesha를 여러 노드로 구성하고, 하나의 노드에 장애가 발생하면 다른 노드에서 계속하여 서비스할 수 있도록 구성합니다. 하나의 NFS-Ganesha 서비스 기준으로는 Active-Backup 구조로 HA가 이루어집니다. Keepalived 설정에서 VIP를 어떻게 구성하느냐에 따라 전체 NFS-Ganesha 서비스는 Active-Active로 구성할 수도 있습니다.

1. YUM Repository 설정

    ```
    vi /etc/yum.repos.d/ganesha.repo
    ```
    > ```
    > [ganesha]
    > name=ganesha packages for $basearch
    > baseurl=https://download.ceph.com/nfs-ganesha/rpm-V2.7-stable/luminous/$basearch
    > enabled=1
    > gpgcheck=1
    > type=rpm-md
    > gpgkey=https://download.ceph.com/keys/release.asc
    > 
    > [ganesha-noarch]
    > name=ganesha noarch packages
    > baseurl=https://download.ceph.com/nfs-ganesha/rpm-V2.7-stable/luminous/noarch
    > enabled=1
    > gpgcheck=1
    > type=rpm-md
    > gpgkey=https://download.ceph.com/keys/release.asc
    > 
    > [ganesha-source]
    > name=ganesha source packages
    > baseurl=https://download.ceph.com/nfs-ganesha/rpm-V2.7-stable/luminous/SRPMS
    > enabled=1
    > gpgcheck=1
    > type=rpm-md
    > gpgkey=https://download.ceph.com/keys/release.asc
    > ```

2. 패키지 설치

    ```
    yum install nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace
    ```

3. Ganasha 설정

    설정 파일에 손을 대지 않아도 단일 구성에서는 문제 없이 동작합니다. HA 관점에서 보면 `RADOS_URLS`부터 `RADOS_KV`까지가 핵심입니다.

    ```
    vi /etc/ganesha/ganesha.conf
    ```
    > ```
    > ###################################################
    > #
    > # Ganesha Config Example
    > #
    > # This is a commented example configuration file for Ganesha.  It is not
    > # complete, but only has some common configuration options.  See the man pages
    > # for complete documentation.
    > #
    > ###################################################
    > 
    > ## These are core parameters that affect Ganesha as a whole.
    > NFS_CORE_PARAM {
    > 	## Allow NFSv3 to mount paths with the Pseudo path, the same as NFSv4,
    > 	## instead of using the physical paths.
    > 	#mount_path_pseudo = true;
    > 
    > 	## Configure the protocols that Ganesha will listen for.  This is a hard
    > 	## limit, as this list determines which sockets are opened.  This list
    > 	## can be restricted per export, but cannot be expanded.
    > 	#Protocols = 3,4,9P;
    > 	Protocols = 4;
    > 	Enable_RQUOTA = false;
    > }
    > 
    > RADOS_URLS {
    >     ceph_conf = "/etc/ceph/ceph.conf";
    >     userid = "admin";
    > }
    > 
    > %url "rados://cephfs_data/ganesha-export-index"
    > 
    > NFSv4 {
    >     RecoveryBackend = "rados_cluster";
    >     Grace_Period = 60;
    >     Lease_Lifetime = 5;
    >     Minor_Versions = 1,2;
    > }
    > 
    > RADOS_KV {
    >     ceph_conf = "/etc/ceph/ceph.conf";
    >     userid = "admin";
    >     pool = "cephfs_data";
    >     namespace = "ganesha";
    >     # Some Unique ID for each NFS-Ganesha node
    >     nodeid = "GANESHA_NODE_ID";
    > }
    > 
    > ## These are defaults for exports.  They can be overridden per-export.
    > #EXPORT_DEFAULTS {
    > 	## Access type for clients.  Default is None, so some access must be
    > 	## given either here or in the export itself.
    > 	#Access_Type = RW;
    > #}
    > 
    > ## Configure settings for the object handle cache
    > #CACHEINODE {
    > 	## The point at which object cache entries will start being reused.
    > 	#Entries_HWMark = 100000;
    > #}
    > 
    > ## Configure an export for some file tree
    > #EXPORT
    > #{
    > 	## Export Id (mandatory, each EXPORT must have a unique Export_Id)
    > 	#Export_Id = 12345;
    > 
    > 	## Exported path (mandatory)
    > 	#Path = /nonexistant;
    > 
    > 	## Pseudo Path (required for NFSv4 or if mount_path_pseudo = true)
    > 	#Pseudo = /nonexistant;
    > 
    > 	## Restrict the protocols that may use this export.  This cannot allow
    > 	## access that is denied in NFS_CORE_PARAM.
    > 	#Protocols = 3,4;
    > 
    > 	## Access type for clients.  Default is None, so some access must be
    > 	## given. It can be here, in the EXPORT_DEFAULTS, or in a CLIENT block
    > 	#Access_Type = RW;
    > 
    > 	## Whether to squash various users.
    > 	#Squash = root_squash;
    > 
    > 	## Allowed security types for this export
    > 	#Sectype = sys,krb5,krb5i,krb5p;
    > 
    > 	## Exporting FSAL
    > 	#FSAL {
    > 		#Name = VFS;
    > 	#}
    > #}
    > 
    > # Configure logging.  Default is to log to Syslog.  Basic logging can also be
    > # configured from the command line
    > LOG {
    > 	# Default log level for all components
    > 	Default_Log_Level = INFO;
    > 
    > 	# Configure per-component log levels.
    > 	Components {
    > 		FSAL = INFO;
    > 		NFS4 = EVENT;
    > 	}
    > 
    > 	# Where to log
    > 	Facility {
    > 		name = FILE;
    > 		destination = "/var/log/ganesha/ganesha.log";
    > 		enable = active;
    > 	}
    > }
    > ```

4. HA를 위한 설정

    - NFS Export 정보 저장을 위한 파일 생성

        ```
        echo | rados -p cephfs_data put ganesha-export-index -
        ```

    - NFS-Ganesha 호스트 사이에 정보 공유를 하기 위한 설정

        ```
        ganesha-rados-grace -p cephfs_data -n ganesha add GANESHA_NODE_ID1
        ganesha-rados-grace -p cephfs_data -n ganesha add GANESHA_NODE_ID2
        ganesha-rados-grace -p cephfs_data -n ganesha add GANESHA_NODE_ID3
        ```

5. HA를 위한 VIP 설정 (Keepalived)

    - 패키지 설치
    
        ```
        yum install keepalived
        ```
    
    - Keepalived 설정

        ```
        vi /etc/keepalived/keepalived/conf
        ```
        > ```
        > vrrp_script ganesha_check_script {
        >   script "/bin/kill -0 $(cat /var/run/ganesha.pid)"
        >   interval 5
        >   fall 3
        >   rise 6
        > }
        > 
        > vrrp_instance ganesha {
        >   interface NFS_SERVICE_IFACE
        >   state BACKUP
        >   nopreempt
        >   virtual_router_id 20
        >   priority 50
        >   authentication {
        >     auth_type PASS
        >     auth_pass KEEPALIVED_AUTHPASS
        >   }
        >   virtual_ipaddress {
        >     NFS_SERVICE_VIP_CIDR dev NFS_SERVICE_IFACE
        >   }
        >   track_script {
        >     ganesha_check_script
        >   }
        > }
        > ```

6. 서비스 기동

    - Keepalived
        ```
        systemctl start keepalived
        systemctl enable keepalived
        ```
      
    - NFS-Ganesha
      
        ```
        systemctl start nfs-ganesha
        systemctl enable nfs-ganesha
        ```

    - Manila

        ```
        systemctl start manila-api manila-scheduler manila-share
        systemctl enable manila-api manila-scheduler manila-share
        ```

# 참고문서

- Manila 설치 공식 매뉴얼

    OpenStack Docs: Installation Tutorial <br/>
    https://docs.openstack.org/manila/queens/install/index.html

- OpenStack Ansible의 Manila 설치 모듈

    Role os_manila for OpenStack-Ansible <br/>
    https://opendev.org/openstack/openstack-ansible-os_manila/src/branch/stable/stein

- NFS-Genasha HA 구성 관련 포스트

    Deploying an Active/Active NFS Cluster over CephFS | Jeff Layton <br/>
    https://jtlayton.wordpress.com/2018/12/10/deploying-an-active-active-nfs-cluster-over-cephfs/

- RADOS_KV 백엔드 사용 관련 글타래

    [ceph-users] Nfs-ganesha with rados_kv backend <br/>
    http://lists.ceph.com/pipermail/ceph-users-ceph.com/2019-May/035093.html
